{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on_kaggle = True\n",
    "on_kaggle = False\n",
    "\n",
    "#TRAIN_PREDICT = 'predict'\n",
    "TRAIN_PREDICT = 'train'\n",
    "\n",
    "if not on_kaggle:\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "import random\n",
    "\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "print(tf.__version__)\n",
    "\n",
    "#fix bug with using CuDNNLSTM\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "from transformers import *\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    #torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed_all(seed)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read data and tokenizer\n",
    "\n",
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n",
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer', 'category', 'host']\n"
     ]
    }
   ],
   "source": [
    "if on_kaggle:\n",
    "    PATH = '../input/google-quest-challenge/'\n",
    "    BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\n",
    "    CKPT_LOAD_PATH = ['../input/google-quest-qa-labeling-checkpoints-v3/fold0-epoch3.h5py', \n",
    "                      '../input/google-quest-qa-labeling-checkpoints-v3/fold1-epoch3.h5py',\n",
    "                      '../input/google-quest-qa-labeling-checkpoints-v3/fold2-epoch3.h5py',\n",
    "                      '../input/google-quest-qa-labeling-checkpoints-v3/fold3-epoch3.h5py',\n",
    "                      '../input/google-quest-qa-labeling-checkpoints-v3/fold4-epoch3.h5py'\n",
    "                     ]\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "else:##offline\n",
    "    PATH = '../data/'\n",
    "    BERT_PATH = '../model/xlnet-base-cased-v3'\n",
    "    CKPT_SAVE_PATH = '../checkpoint/xlnet-base-cased-v3/'\n",
    "    CKPT_LOAD_PATH = ['../checkpoint/xlnet-base-cased/']\n",
    "    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5,9,10]])#9:category, 10:host\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing functions\n",
    "\n",
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t,q,a = df_train[input_categories].iloc[3].to_list()\n",
    "# ids = tokenizer.encode_plus(a, None, 'longest_first', max_length=None)\n",
    "\n",
    "# input_ids = ids['input_ids']\n",
    "# len(input_ids)\n",
    "\n",
    "#input_segments = ids['token_type_ids']\n",
    "# for i in range(5):\n",
    "#     t,q,a = df_train.loc[i, ['question_title', 'question_body', 'answer']]\n",
    "#     print(t)\n",
    "#     print('-'*10)\n",
    "#     print(q)\n",
    "#     print('-'*10)\n",
    "#     print(a)\n",
    "#     print('='*40)\n",
    "\n",
    "# t = df_train.loc[df_train.host=='stackoverflow.com', 'question_title'].values[11]\n",
    "# q = df_train.loc[df_train.host=='stackoverflow.com', 'question_body'].values[11]\n",
    "\n",
    "# tokens = tokenizer.encode_plus(t+' '+q, None, add_special_tokens=True, max_length=512, truncation_strategy='longest_first')\n",
    "# tokens['token_type_ids']\n",
    "# print(t+'END OF TITLE\\n'+q)\n",
    "# print(' '.join(tokenizer.ids_to_tokens[i] for i in tokens['input_ids']))\n",
    "# [tokenizer.ids_to_tokens[i] for i in tokens['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_special_char(x):\n",
    "    special_character = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]', re.IGNORECASE)\n",
    "    x_ascii = unidecode(x)\n",
    "    s = special_character.findall(x_ascii)\n",
    "    #print(s)\n",
    "    c = len(s)\n",
    "    return c\n",
    "\n",
    "def count_cap_char(x):\n",
    "    c = sum(1 for l in x if l.isupper())\n",
    "    return c\n",
    "\n",
    "def count_unique_words(x):\n",
    "    \"\"\"returns a ratio\"\"\"\n",
    "    special_character = re.compile(r'[^A-Za-z0-9]', re.IGNORECASE)\n",
    "    a = [w for w in special_character.split(x) if w!='']\n",
    "    r = len(set(a))/len(a)\n",
    "    return r\n",
    "\n",
    "#print(t)\n",
    "#count_cap_char(t), len(t)\n",
    "#count_special_char(t), len(t)\n",
    "#string.printable\n",
    "\n",
    "#from textblob import TextBlob\n",
    "# print(t.lower())\n",
    "# print('='*40)\n",
    "# print(TextBlob(t.lower()).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIFE_ARTS': 1,\n",
       " 'CULTURE': 2,\n",
       " 'SCIENCE': 3,\n",
       " 'STACKOVERFLOW': 4,\n",
       " 'TECHNOLOGY': 5,\n",
       " 'UNK': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = df_train.category.unique().tolist()\n",
    "category2index = dict([(l[i],i+1) for i in range(len(l))])\n",
    "category2index['UNK'] = 0\n",
    "\n",
    "l = df_train.host.unique().tolist()\n",
    "host2index = dict([(l[i],i+1) for i in range(len(l))])\n",
    "host2index['UNK'] = 0\n",
    "\n",
    "category2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BucketedDataIterator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    tr = BucketedDataIterator(df_train_trans, num_buckets=10, testset=False)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, num_buckets=10, testset=False):\n",
    "        df = df.sort_values('qa_len').reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.size = len(df) / num_buckets\n",
    "        self.dfs = []\n",
    "        for bucket in range(num_buckets):\n",
    "            self.dfs.append(df.loc[bucket*self.size: (bucket+1)*self.size - 1])\n",
    "        self.num_buckets = num_buckets\n",
    "\n",
    "        # cursor[i] will be the cursor for the ith bucket\n",
    "        self.cursor = np.array([0] * num_buckets)\n",
    "        self.shuffle()\n",
    "\n",
    "        self.epochs = 0\n",
    "        \n",
    "        self.testset = testset\n",
    "\n",
    "    def shuffle(self):\n",
    "        #sorts dataframe by sequence length, but keeps it random within the same length\n",
    "        for i in range(self.num_buckets):\n",
    "            self.dfs[i] = self.dfs[i].sample(frac=1).reset_index(drop=True)\n",
    "            self.cursor[i] = 0\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.cursor = np.array([0] * num_buckets)\n",
    "        self.shuffle()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return ceil(len(self.df) / BATCH_SIZE)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        to_choose = []\n",
    "        for i in range(self.num_buckets):\n",
    "            cursor = self.cursor[i]+BATCH_SIZE+1\n",
    "            if cursor < self.size:\n",
    "                to_choose.append(i)\n",
    "        if to_choose!=[]:\n",
    "            i = np.random.choice(to_choose, 1)[0]\n",
    "            #print(i)\n",
    "        else:\n",
    "            i = np.random.choice(list(range(self.num_buckets)), 1)[0]\n",
    "            self.shuffle()\n",
    "            #print('Empty list Error\\ni = np.random.choice(to_choose, 1)[0]\\n')\n",
    "            #print(i)\n",
    "\n",
    "        res = self.dfs[i].loc[self.cursor[i]:self.cursor[i]+BATCH_SIZE-1]\n",
    "        self.cursor[i] += BATCH_SIZE\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        #qa_tokens, qa_masks, qa_segments\n",
    "        maxlen = max(res['qa_len'])\n",
    "        tokens = np.zeros([BATCH_SIZE, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(tokens):\n",
    "            x_i[:res['qa_len'].values[i]] = res['qa_tokens'].values[i]\n",
    "        masks = np.zeros([BATCH_SIZE, maxlen], dtype=np.float32)\n",
    "        for i, x_i in enumerate(masks):\n",
    "            x_i[:res['qa_len'].values[i]] = res['qa_masks'].values[i]\n",
    "        segments = np.zeros([BATCH_SIZE, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(segments):\n",
    "            x_i[:res['qa_len'].values[i]] = res['qa_segments'].values[i]\n",
    "        if maxlen>1024:\n",
    "            tokens = tokens[:1024]\n",
    "            masks = masks[:1024]\n",
    "            segments = segments[:1024]\n",
    "        \n",
    "#         if idx==0:\n",
    "#             print(np.array([tokens, masks, segments]))\n",
    "#             print('='*40)\n",
    "#             print(res[output_categories].values)\n",
    "        \n",
    "        if self.testset:\n",
    "            return [tokens, masks, segments]\n",
    "        return [tokens, masks, segments], [res[output_categories].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 288), (8, 30), 760, array([0, 0, 0, 8, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = BucketedDataIterator(df_train_trans)\n",
    "x,y = tr[0]\n",
    "x[0].shape, y[0].shape, len(tr), tr.cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _convert_to_transformer_inputs(title, question, answer, tokenizer):\n",
    "#     \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\n",
    "    \n",
    "#     NOTE: USE Head + Tail truncation\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def return_id(str1, None):\n",
    "\n",
    "#         inputs = tokenizer.encode_plus(str1, None,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=None,\n",
    "#             truncation_strategy='longest_first')\n",
    "        \n",
    "#         length = MAX_SEQUENCE_LENGTH\n",
    "        \n",
    "#         input_ids =  inputs[\"input_ids\"]\n",
    "#         input_masks = [1.0] * len(input_ids)\n",
    "#         input_segments = inputs[\"token_type_ids\"]\n",
    "#         if len(input_ids)>length:#Head + Tail truncate\n",
    "#             input_ids = input_ids[:128] + input_ids[-384:]\n",
    "#             input_masks = input_masks[:128] + input_masks[-384:]\n",
    "#             input_segments = input_segments[:128] + input_segments[-384:]\n",
    "#         padding_length = length - len(input_ids)\n",
    "#         padding_id = tokenizer.pad_token_id\n",
    "#         input_ids = input_ids + ([padding_id] * padding_length)\n",
    "#         input_masks = input_masks + ([0.0] * padding_length)\n",
    "#         input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "#         return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "#     input_ids_q, input_masks_q, input_segments_q = return_id(title + ' ' + question, None)\n",
    "    \n",
    "#     input_ids_a, input_masks_a, input_segments_a = return_id(answer, None)\n",
    "\n",
    "#     return [input_ids_q, input_masks_q, input_segments_q,\n",
    "#             input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "# def compute_input_arrays(df, columns, tokenizer):\n",
    "#     input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "#     input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "#     special_char_q, special_char_a = [], []\n",
    "#     cap_char_q, cap_char_a = [], []\n",
    "#     unique_words_q, unique_words_a = [], []\n",
    "#     category_index, host_index = [], []\n",
    "#     for _, instance in tqdm(df[columns].iterrows()):\n",
    "#         t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "        \n",
    "#         ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer)\n",
    "        \n",
    "#         input_ids_q.append(ids_q)\n",
    "#         input_masks_q.append(masks_q)\n",
    "#         input_segments_q.append(segments_q)\n",
    "\n",
    "#         input_ids_a.append(ids_a)\n",
    "#         input_masks_a.append(masks_a)\n",
    "#         input_segments_a.append(segments_a)\n",
    "        \n",
    "#         special_char_q.append(count_special_char(t+q)/len(t+q))\n",
    "#         special_char_a.append(count_special_char(a)/len(a))\n",
    "#         cap_char_q.append(count_cap_char(t+q)/len(t+q))\n",
    "#         cap_char_a.append(count_cap_char(a)/len(a))\n",
    "#         unique_words_q.append(count_unique_words(t+q))\n",
    "#         unique_words_a.append(count_unique_words(a))\n",
    "        \n",
    "#         category, host = instance.category, instance.host\n",
    "#         category_index.append([category2index.get(category, 0)])\n",
    "#         host_index.append([host2index.get(host, 0)])\n",
    "        \n",
    "#     return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "#             np.asarray(input_masks_q, dtype=np.float32), \n",
    "#             np.asarray(input_segments_q, dtype=np.int32),\n",
    "#             np.asarray(input_ids_a, dtype=np.int32), \n",
    "#             np.asarray(input_masks_a, dtype=np.float32), \n",
    "#             np.asarray(input_segments_a, dtype=np.int32),\n",
    "#             np.asarray([special_char_q, special_char_a, cap_char_q, cap_char_a, unique_words_q, unique_words_a], dtype=np.float32).T, \n",
    "#             np.asarray(category_index, dtype=np.int32),\n",
    "#             np.asarray(host_index, dtype=np.int32)\n",
    "#            ]\n",
    "\n",
    "# def compute_output_arrays(df, columns):\n",
    "#     return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010e600864d941058cf54787b63dec6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def make_data_from_df(df, testset=False):\n",
    "    q_len, a_len = [], []\n",
    "    q_tokens, a_tokens = [], []\n",
    "    q_masks, a_masks = [], []\n",
    "    q_segments, a_segments = [], []\n",
    "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer']].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "        ## Question\n",
    "        inputs = tokenizer.encode_plus(t+' '+q, None,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=None,\n",
    "                        truncation_strategy='longest_first')#t+' '+q+' '+a\n",
    "        tokens = inputs[\"input_ids\"]\n",
    "        q_tokens.append(tokens)\n",
    "        q_masks.append([1.0]*len(tokens))\n",
    "        q_segments.append(inputs[\"token_type_ids\"])\n",
    "        q_len.append(len(tokens))\n",
    "        ## Answer\n",
    "        inputs = tokenizer.encode_plus(a, None,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=None,\n",
    "                        truncation_strategy='longest_first')#t+' '+q+' '+a\n",
    "        tokens = inputs[\"input_ids\"]\n",
    "        a_tokens.append(tokens)\n",
    "        a_masks.append([1.0]*len(tokens))\n",
    "        a_segments.append(inputs[\"token_type_ids\"])\n",
    "        a_len.append(len(tokens))\n",
    "\n",
    "    ##\n",
    "    #cols = ['q_tokens', 'q_masks', 'q_segments', 'a_tokens', 'a_masks', 'a_segments', 'q_len', 'a_len'] + \\\n",
    "    #        input_categories + output_categories\n",
    "    output = pd.DataFrame({'q_tokens':q_tokens, 'q_masks':q_masks, 'q_segments':q_segments, 'q_len':q_len, \n",
    "                           'a_tokens':a_tokens, 'a_masks':a_masks, 'a_segments':a_segments, 'a_len':a_len})\n",
    "    if not testset:\n",
    "        for col in output_categories:\n",
    "            output[col] = df[col]\n",
    "    return output\n",
    "\n",
    "df_train_trans = make_data_from_df(df_train)\n",
    "#df_test_trans = make_data_from_df(df_test, testset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.quantile(df_train_trans.q_len.values, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]), \\\n",
    "#np.quantile(df_train_trans.a_len.values, [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95])\n",
    "#df_train_trans.head(5)\n",
    "\n",
    "l = []\n",
    "#df = df_train_trans[['q_len', 'a_len']].sample(frac=1.0)\n",
    "#df = df_train_trans[['q_len', 'a_len']].sort_values(['q_len'], ascending=True)\n",
    "df = df_train_trans[['q_len', 'a_len']].copy()\n",
    "df['qa_len'] = df['q_len'] + df['a_len']\n",
    "df = df.sort_values(['qa_len'], ascending=True)\n",
    "\n",
    "df = df.values\n",
    "for i in range(760):\n",
    "    v = df[i*8:(i+1)*8, :].max(axis=0)\n",
    "    l.append(v)\n",
    "\n",
    "l = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7776315789473685"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l[:,0]<512).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create model\n",
    "\n",
    "`compute_spearmanr()` is used to compute the competition metric for the validation set\n",
    "<br><br>\n",
    "`CustomCallback()` is a class which inherits from `tf.keras.callbacks.Callback` and will compute and append validation score and validation/test predictions respectively, after each epoch.\n",
    "<br><br>\n",
    "`bert_model()` contains the actual architecture that will be used to finetune BERT to our dataset. It's simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)\n",
    "<br><br>\n",
    "`train_and_predict()` this function will be run to train and obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_iter, fold=None, stage2=False):\n",
    "\n",
    "        self.valid_iter = valid_iter\n",
    "        self.fold = fold\n",
    "        self.stage2 = stage2\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict_generator(generator=self.valid_iter)\n",
    "        )\n",
    "        \n",
    "        rho_val = compute_spearmanr_ignore_nan(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "        \n",
    "        print(\"\\nvalidation rho: %.4f\" % rho_val)\n",
    "        \n",
    "        if self.fold is not None and epoch>1:\n",
    "            if self.stage2:\n",
    "                self.model.save_weights(CKPT_SAVE_PATH+f'fold{fold}-epoch{epoch}-stage2.h5py')\n",
    "            else:\n",
    "                self.model.save_weights(CKPT_SAVE_PATH+f'fold{fold}-epoch{epoch}.h5py')\n",
    "\n",
    "\n",
    "# def xlnet_model():\n",
    "#     q_id = tf.keras.layers.Input((None,), dtype=tf.int32)#MAX_SEQUENCE_LENGTH\n",
    "#     a_id = tf.keras.layers.Input((None,), dtype=tf.int32)\n",
    "    \n",
    "#     q_mask = tf.keras.layers.Input((None,), dtype=tf.float32)\n",
    "#     a_mask = tf.keras.layers.Input((None,), dtype=tf.float32)\n",
    "    \n",
    "#     q_atn = tf.keras.layers.Input((None,), dtype=tf.int32)\n",
    "#     a_atn = tf.keras.layers.Input((None,), dtype=tf.int32)\n",
    "    \n",
    "#     feats = tf.keras.layers.Input((6,), dtype=tf.float32)#set dim of additional numeric feats\n",
    "#     category_index = tf.keras.layers.Input((1,), dtype=tf.int32)\n",
    "#     host_index = tf.keras.layers.Input((1,), dtype=tf.int32)\n",
    "    \n",
    "#     #config = XLNetConfig.from_json_file('../data/xlnet-base-cased/xlnet-base-cased-config.json')\n",
    "#     #config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "#     # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "#     # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "#     # pretrained model has been downloaded manually and uploaded to kaggle.\n",
    "#     #base_model = bert_base_model()\n",
    "#     #base_model.load_weights(CKPT_SAVE_PATH+f'fold{fold}-epoch3.h5py')\n",
    "#     #bert_weights = base_model.layers[8].get_weights()\n",
    "#     if on_kaggle:\n",
    "#         roberta_layer = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "#     else:\n",
    "#         xlnet_layer = TFXLNetModel.from_pretrained('xlnet-base-cased')#config=config\n",
    "#     #bert_layer.set_weights(bert_weights)\n",
    "#     #bert_layer.trainable = False\n",
    "    \n",
    "#     # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "#     q_embedding = xlnet_layer(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "#     a_embedding = xlnet_layer(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "#     #q_embedding = tf.keras.layers.SpatialDropout1D(0.2)(q_embedding)\n",
    "#     #a_embedding = tf.keras.layers.SpatialDropout1D(0.2)(a_embedding)\n",
    "    \n",
    "#     qa_embedding = tf.keras.layers.Concatenate()([q_embedding, a_embedding])\n",
    "#     qa1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True, \n",
    "#                                                             activation='tanh', recurrent_activation='sigmoid', \n",
    "#                                                             recurrent_dropout=0, unroll=False, use_bias=False\n",
    "#                                                             ))(qa_embedding)\n",
    "#     qa2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, return_sequences=True, \n",
    "#                                                             activation='tanh', recurrent_activation='sigmoid', \n",
    "#                                                             recurrent_dropout=0, unroll=False, use_bias=False\n",
    "#                                                             ))(qa1)\n",
    "    \n",
    "#     qa1_avgpool = tf.keras.layers.GlobalAveragePooling1D()(qa1)\n",
    "#     qa2_avgpool = tf.keras.layers.GlobalAveragePooling1D()(qa2)\n",
    "#     #qa_maxpool = tf.keras.layers.GlobalMaxPool1D()(qa)\n",
    "#     q_avgpool = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "#     a_avgpool = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "#     category_embed_layer = tf.keras.layers.Embedding(6, 8, input_length=1)\n",
    "#     host_embed_layer = tf.keras.layers.Embedding(64, 8, input_length=1)\n",
    "#     cat_embed = category_embed_layer(category_index)\n",
    "#     host_embed = host_embed_layer(host_index)\n",
    "#     cat_embed = tf.keras.layers.Flatten()(cat_embed)\n",
    "#     host_embed = tf.keras.layers.Flatten()(host_embed)\n",
    "    \n",
    "#     #x = tf.keras.layers.Concatenate()([q, a, feats, cat_embed, host_embed])\n",
    "#     x = tf.keras.layers.Concatenate()([qa1_avgpool, qa2_avgpool, q_avgpool, a_avgpool,\n",
    "#                                        feats, cat_embed, host_embed])\n",
    "    \n",
    "#     x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "#     model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, \n",
    "#                                           feats, category_index, host_index, ], outputs=x)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "def xlnet_model():\n",
    "    qa_id = tf.keras.layers.Input((None,), dtype=tf.int32)#MAX_SEQUENCE_LENGTH\n",
    "    qa_mask = tf.keras.layers.Input((None,), dtype=tf.float32)\n",
    "    qa_atn = tf.keras.layers.Input((None,), dtype=tf.int32)\n",
    "    \n",
    "    if on_kaggle:\n",
    "        roberta_layer = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    else:\n",
    "        xlnet_layer = TFXLNetModel.from_pretrained('xlnet-base-cased')#config=config\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    qa_embedding = xlnet_layer(qa_id, attention_mask=qa_mask, token_type_ids=qa_atn)[0]\n",
    "    \n",
    "    #qa_embedding = tf.keras.layers.Concatenate()([q_embedding, a_embedding])\n",
    "    qa1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=True, \n",
    "                                                            activation='tanh', recurrent_activation='sigmoid', \n",
    "                                                            recurrent_dropout=0, unroll=False, use_bias=False\n",
    "                                                            ))(qa_embedding)\n",
    "    qa2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(512, return_sequences=True, \n",
    "                                                            activation='tanh', recurrent_activation='sigmoid', \n",
    "                                                            recurrent_dropout=0, unroll=False, use_bias=False\n",
    "                                                            ))(qa1)\n",
    "    \n",
    "    qa1_avgpool = tf.keras.layers.GlobalAveragePooling1D()(qa1)\n",
    "    qa2_avgpool = tf.keras.layers.GlobalAveragePooling1D()(qa2)\n",
    "    #qa_maxpool = tf.keras.layers.GlobalMaxPool1D()(qa)\n",
    "    qa_avgpool = tf.keras.layers.GlobalAveragePooling1D()(qa_embedding)\n",
    "        \n",
    "    x = tf.keras.layers.Concatenate()([qa1_avgpool, qa2_avgpool, qa_avgpool])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[qa_id, qa_mask, qa_atn, ], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train(model, train_iter, valid_iter, \n",
    "          learning_rate, epochs, batch_size, loss_function, fold, stage2=False):\n",
    "    \"\"\"\n",
    "    multipliers = {'dense_1': 0.5, 'dense_2': 0.4}\n",
    "    optimizer = LearningRateMultiplier(tf.keras.optimizers.Adam, \n",
    "                                        lr_multiplier=multipliers, learning_rate=learning_rate)\n",
    "    \n",
    "    \"\"\"\n",
    "    custom_callback = CustomCallback(\n",
    "        valid_iter=valid_iter, \n",
    "        fold=fold, stage2=stage2)\n",
    "\n",
    "    #decay_steps = 1000\n",
    "    #lr_decayed_fn = tf.keras.experimental.CosineDecay(learning_rate, decay_steps)\n",
    "    #clr = CyclicLR(base_lr=2e-5, max_lr=5e-5, step_size=2000., mode='triangular')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)#clipvalue=0.5\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    model.fit_generator(generator=train_iter, epochs=epochs, \n",
    "              callbacks=[custom_callback], shuffle=False)#batch_size=batch_size\n",
    "    \n",
    "    return custom_callback\n",
    "\n",
    "def predict(model, test_data, load_weights_path):\n",
    "    model.load_weights(load_weights_path)\n",
    "    return model.predict(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tfxl_net_model_1 (TFXLNetModel) ((None, None, 768),) 116718336   input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 2048)   14680064    tfxl_net_model_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, None, 1024)   7864320     bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 2048)         0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 1024)         0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 768)          0           tfxl_net_model_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3840)         0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 3840)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           115230      dropout_75[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 139,377,950\n",
      "Trainable params: 139,377,950\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#config = RobertaConfig()\n",
    "#print(config)\n",
    "#config.output_hidden_states = False\n",
    "#xlnet_layer = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "model = xlnet_model()\n",
    "model.summary()\n",
    "\n",
    "#base_model.load_weights('../checkpoint/bert-base-uncased-v3/fold0-epoch3.h5py')\n",
    "\n",
    "#bert_weights = base_model.layers[8].get_weights()\n",
    "\n",
    "#new_bert_layer = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "#new_bert_layer.set_weights(bert_weights)\n",
    "\n",
    "#inputs[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if TRAIN_PREDICT == 'train':\n",
    "#     outputs = compute_output_arrays(df_train, output_categories)\n",
    "#     inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load gkf\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#bug: if create for the first time, need to run twice to load it\n",
    "if TRAIN_PREDICT == 'train':\n",
    "    if not os.path.isfile('../data/gkf%d.pkl'%SEED):\n",
    "        print('Create gkf')\n",
    "        gkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "        with open('../data/gkf%d.pkl'%SEED, 'wb') as f:\n",
    "            pickle.dump(list(gkf), f)\n",
    "    else:\n",
    "        print('Load gkf')\n",
    "        with open('../data/gkf%d.pkl'%SEED, 'rb') as f:\n",
    "            gkf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(inputs), inputs[6].shape, inputs[7], inputs[8])\n",
    "#inputs[6][:,4:]\n",
    "# for i,(train_idx,val_idx) in enumerate(gkf):\n",
    "#     if i==0:\n",
    "#         break\n",
    "# train_idx[20:30]\n",
    "\n",
    "#gkf0 = list(gkf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training, validation and testing\n",
    "\n",
    "Loops over the folds in gkf and trains each fold for 5 epochs --- with a learning rate of 1e-5 and batch_size of 8. A simple binary crossentropy is used as the objective-/loss-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LearningRate = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_bce_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    nunique = [df_train[col].nunique() for col in output_categories]#count unique values of each column\n",
    "    weights_dict = {5:0.5, 9:1.0, 17:1.5, 3:0.5}\n",
    "    weights = [weights_dict[i] for i in nunique]\n",
    "    \"\"\"\n",
    "    weights = tf.convert_to_tensor([1. , 1. , 0.5, 0.5, 0.5, 0.5, 1. , 1. , 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1. , 1. , 1. , 1. , 1. , 1.5,\n",
    "       0.5, 0.5, 0.5, 1. ], dtype=float)\n",
    "    weights = weights/K.mean(weights)\n",
    "    loss = tf.multiply(y_true, K.log(y_pred+K.epsilon())) + tf.multiply((1-y_true), K.log(1-y_pred+K.epsilon()))\n",
    "    loss = tf.multiply(loss, weights)\n",
    "    bce_loss = tf.reduce_mean(-loss)\n",
    "    return bce_loss\n",
    "#     y_true_clip = K.clip(y_true, K.epsilon(), 1)\n",
    "#     y_pred_clip = K.clip(y_pred, K.epsilon(), 1)\n",
    "#     kl_loss = tf.reduce_mean(tf.reduce_sum(y_true_clip * K.log(y_true_clip / y_pred_clip), axis=0))\n",
    "#     return bce_loss*0.9 + kl_loss*0.1\n",
    "\n",
    "#y_pred = tf.random.uniform((8, 30))\n",
    "#y_true = tf.random.uniform((8, 30))\n",
    "\n",
    "#custom_bce_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========training fold 0========\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[623,623,8,12] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AddV2] name: model/tfxl_net_model/transformer/layer_._6/rel_attn/add/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-02e2104e834e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                             \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_bce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                             stage2=False)#'binary_crossentropy'\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5d742b4f0df0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_iter, valid_iter, learning_rate, epochs, batch_size, loss_function, fold, stage2)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     model.fit_generator(generator=train_iter, epochs=epochs, \n\u001b[0;32m--> 176\u001b[0;31m               callbacks=[custom_callback], shuffle=False)#batch_size=batch_size\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, training)\u001b[0m\n\u001b[1;32m    646\u001b[0m             outputs = layer_module([output_h, output_g, non_tgt_mask, attn_mask,\n\u001b[1;32m    647\u001b[0m                                     \u001b[0mpos_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m                                     head_mask[i]], training=training)\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    268\u001b[0m             attn_vec = self.rel_attn_core(\n\u001b[1;32m    269\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mq_head_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_head_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_head_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_head_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 training=training)\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_xlnet.py\u001b[0m in \u001b[0;36mrel_attn_core\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# merge attention scores and perform masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mattn_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mef\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    544\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[623,623,8,12] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AddV2] name: model/tfxl_net_model/transformer/layer_._6/rel_attn/add/"
     ]
    }
   ],
   "source": [
    "## training\n",
    "## if stage2, modify 2 places\n",
    "if TRAIN_PREDICT == 'train':\n",
    "    histories = []\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "\n",
    "        # will actually only do 1 fold (out of 5) to compare models\n",
    "        if fold ==0:\n",
    "            print('========training fold %d========'%fold)\n",
    "            K.clear_session()\n",
    "            \n",
    "            ##stage 1, finetuning bert as embedding\n",
    "            #model = bert_base_model()\n",
    "            ##stage 2, load bert weights from stage 1, train full model\n",
    "            model = xlnet_model()\n",
    "\n",
    "            #prepare dataset\n",
    "#             train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "#             train_outputs = outputs[train_idx]\n",
    "#             valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "#             valid_outputs = outputs[valid_idx]\n",
    "            train_iter = BucketedDataIterator(df_train_trans.loc[train_idx, ], num_buckets=10, testset=False)\n",
    "            valid_iter = BucketedDataIterator(df_train_trans.loc[valid_idx, ], num_buckets=10, testset=False)\n",
    "\n",
    "            # history contains two lists of valid and test preds respectively:\n",
    "            #  [valid_predictions_{fold}, test_predictions_{fold}]\n",
    "            history = train(model, \n",
    "                            train_iter=train_iter, \n",
    "                            valid_iter=valid_iter, \n",
    "                            learning_rate=LearningRate, epochs=NUM_EPOCHS, batch_size=None,\n",
    "                            loss_function=custom_bce_loss, fold=fold, \n",
    "                            stage2=False)#'binary_crossentropy'\n",
    "            histories.append(history)\n",
    "\n",
    "# ## training full trainset to lift LB score in the final\n",
    "# if TRAIN_PREDICT == 'train':\n",
    "#     histories = []\n",
    "#     print('========Start training========')\n",
    "#     K.clear_session()\n",
    "#     model = bert_model()\n",
    "\n",
    "#     train_inputs = inputs\n",
    "#     train_outputs = outputs\n",
    "\n",
    "#     history = train_and_predict(model, \n",
    "#                       train_data=(train_inputs, train_outputs), \n",
    "#                       valid_data=None,\n",
    "#                       test_data=None, \n",
    "#                       learning_rate=LearningRate, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "#                       loss_function='binary_crossentropy', fold=fold)\n",
    "\n",
    "#     histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCV history\\n---------------\\n#### bert-base ####\\nEpoch 5/15\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3091\\nvalidation rho: 0.3923\\n4863/4863 [==============================] - 339s 70ms/sample - loss: 0.3091\\n------LB=0.346\\n\\nswitch to HuggingFace\\n----------------------\\nSEEMS DEPENDS ON THE SEED!!!\\n----------------------------\\nt + q[:1/2], q[1/2:], a\\nsame\\n\\ncategory + host + t + q, category + host + a\\nsame\\n\\n\\nCUSTOM LOSS\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3612\\nvalidation rho: 0.3989\\n4863/4863 [==============================] - 711s 146ms/sample - loss: 0.3612\\n\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3597\\nvalidation rho: 0.3998\\n4863/4863 [==============================] - 724s 149ms/sample - loss: 0.3597\\n\\nadd 2 feats\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3616\\nvalidation rho: 0.4049\\n4863/4863 [==============================] - 718s 148ms/sample - loss: 0.3616\\n\\n4 feats\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3592\\nvalidation rho: 0.4027\\n4863/4863 [==============================] - 706s 145ms/sample - loss: 0.3591\\n\\nadd cat+host embed dim=16\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3594\\nvalidation rho: 0.4005\\n4863/4863 [==============================] - 698s 144ms/sample - loss: 0.3594\\n\\nembed dim=8\\nEpoch 4/4\\n4856/4863 [============================>.] - ETA: 0s - loss: 0.3592\\nvalidation rho: 0.4071\\n4863/4863 [==============================] - 734s 151ms/sample - loss: 0.3592\\n\\nadd LSTM features\\nvalidation rho: 0.4081 --fold0\\nvalidation rho: 0.4050 --fold1\\nvalidation rho: 0.4138 --fold2\\n\\n\\n--------\\nepochs progression:\\n\\nvalidation rho: 0.3810\\nvalidation rho: 0.3974\\nvalidation rho: 0.4035\\nvalidation rho: 0.4057\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#more ideas todo: \n",
    "#1.1 modify model\n",
    "#1.2 OOV words spelling correction\n",
    "#2. loss, ranking loss?\n",
    "#3. add custom new tokens?(e.g stackoverflow)\n",
    "#4. roberta?alxnet?\n",
    "#5. RankGauss average folds?\n",
    "#6. freeze some layers of bert?\n",
    "\"\"\"\n",
    "CV history\n",
    "---------------\n",
    "#### bert-base ####\n",
    "Epoch 5/15\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3091\n",
    "validation rho: 0.3923\n",
    "4863/4863 [==============================] - 339s 70ms/sample - loss: 0.3091\n",
    "------LB=0.346\n",
    "\n",
    "switch to HuggingFace\n",
    "----------------------\n",
    "SEEMS DEPENDS ON THE SEED!!!\n",
    "----------------------------\n",
    "t + q[:1/2], q[1/2:], a\n",
    "same\n",
    "\n",
    "category + host + t + q, category + host + a\n",
    "same\n",
    "\n",
    "\n",
    "CUSTOM LOSS\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3612\n",
    "validation rho: 0.3989\n",
    "4863/4863 [==============================] - 711s 146ms/sample - loss: 0.3612\n",
    "\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3597\n",
    "validation rho: 0.3998\n",
    "4863/4863 [==============================] - 724s 149ms/sample - loss: 0.3597\n",
    "\n",
    "add 2 feats\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3616\n",
    "validation rho: 0.4049\n",
    "4863/4863 [==============================] - 718s 148ms/sample - loss: 0.3616\n",
    "\n",
    "4 feats\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3592\n",
    "validation rho: 0.4027\n",
    "4863/4863 [==============================] - 706s 145ms/sample - loss: 0.3591\n",
    "\n",
    "add cat+host embed dim=16\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3594\n",
    "validation rho: 0.4005\n",
    "4863/4863 [==============================] - 698s 144ms/sample - loss: 0.3594\n",
    "\n",
    "embed dim=8\n",
    "Epoch 4/4\n",
    "4856/4863 [============================>.] - ETA: 0s - loss: 0.3592\n",
    "validation rho: 0.4071\n",
    "4863/4863 [==============================] - 734s 151ms/sample - loss: 0.3592\n",
    "\n",
    "add LSTM features\n",
    "validation rho: 0.4081 --fold0\n",
    "validation rho: 0.4050 --fold1\n",
    "validation rho: 0.4138 --fold2\n",
    "\n",
    "\n",
    "--------\n",
    "epochs progression:\n",
    "\n",
    "validation rho: 0.3810\n",
    "validation rho: 0.3974\n",
    "validation rho: 0.4035\n",
    "validation rho: 0.4057\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Process and submit test predictions\n",
    "\n",
    "First the test predictions are read from the list of lists of `histories`. Then each test prediction list (in lists) is averaged. Then a mean of the averages is computed to get a single prediction for each data point. Finally, this is saved to `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c558836a72e49079836e4a6a43bdd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_PREDICT == 'predict':\n",
    "    \n",
    "    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    model = bert_model()\n",
    "    \n",
    "    test_predictions = [predict(model, test_inputs, load_weights_path=ckpt_load_path) \n",
    "                        for ckpt_load_path in CKPT_LOAD_PATH]\n",
    "    #test_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\n",
    "    test_predictions = np.mean(test_predictions, axis=0)\n",
    "\n",
    "    df_sub.iloc[:, 1:] = test_predictions\n",
    "\n",
    "    df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.950409</td>\n",
       "      <td>0.667482</td>\n",
       "      <td>0.239416</td>\n",
       "      <td>0.339715</td>\n",
       "      <td>0.570079</td>\n",
       "      <td>0.466379</td>\n",
       "      <td>0.700281</td>\n",
       "      <td>0.700085</td>\n",
       "      <td>0.571651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939537</td>\n",
       "      <td>0.927281</td>\n",
       "      <td>0.502933</td>\n",
       "      <td>0.972254</td>\n",
       "      <td>0.975164</td>\n",
       "      <td>0.780713</td>\n",
       "      <td>0.031082</td>\n",
       "      <td>0.050007</td>\n",
       "      <td>0.861596</td>\n",
       "      <td>0.923908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.859322</td>\n",
       "      <td>0.471540</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.772744</td>\n",
       "      <td>0.760206</td>\n",
       "      <td>0.914244</td>\n",
       "      <td>0.574795</td>\n",
       "      <td>0.459414</td>\n",
       "      <td>0.150252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614215</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.622804</td>\n",
       "      <td>0.984339</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>0.901024</td>\n",
       "      <td>0.941507</td>\n",
       "      <td>0.119569</td>\n",
       "      <td>0.096480</td>\n",
       "      <td>0.902197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.908096</td>\n",
       "      <td>0.620519</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.787047</td>\n",
       "      <td>0.934155</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>0.608684</td>\n",
       "      <td>0.413993</td>\n",
       "      <td>0.300243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878956</td>\n",
       "      <td>0.914537</td>\n",
       "      <td>0.562891</td>\n",
       "      <td>0.970561</td>\n",
       "      <td>0.969091</td>\n",
       "      <td>0.786442</td>\n",
       "      <td>0.036871</td>\n",
       "      <td>0.066868</td>\n",
       "      <td>0.924306</td>\n",
       "      <td>0.903338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.894989</td>\n",
       "      <td>0.414773</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.709674</td>\n",
       "      <td>0.744439</td>\n",
       "      <td>0.904797</td>\n",
       "      <td>0.542779</td>\n",
       "      <td>0.426597</td>\n",
       "      <td>0.098071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731927</td>\n",
       "      <td>0.948509</td>\n",
       "      <td>0.699444</td>\n",
       "      <td>0.967164</td>\n",
       "      <td>0.983113</td>\n",
       "      <td>0.905921</td>\n",
       "      <td>0.857659</td>\n",
       "      <td>0.183874</td>\n",
       "      <td>0.640012</td>\n",
       "      <td>0.921505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.921447</td>\n",
       "      <td>0.422136</td>\n",
       "      <td>0.035422</td>\n",
       "      <td>0.858205</td>\n",
       "      <td>0.736690</td>\n",
       "      <td>0.841480</td>\n",
       "      <td>0.635920</td>\n",
       "      <td>0.601400</td>\n",
       "      <td>0.237748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662975</td>\n",
       "      <td>0.926064</td>\n",
       "      <td>0.677840</td>\n",
       "      <td>0.978456</td>\n",
       "      <td>0.973675</td>\n",
       "      <td>0.848344</td>\n",
       "      <td>0.341229</td>\n",
       "      <td>0.161605</td>\n",
       "      <td>0.517119</td>\n",
       "      <td>0.905178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>245</td>\n",
       "      <td>0.949044</td>\n",
       "      <td>0.838918</td>\n",
       "      <td>0.037897</td>\n",
       "      <td>0.670364</td>\n",
       "      <td>0.950395</td>\n",
       "      <td>0.876493</td>\n",
       "      <td>0.649106</td>\n",
       "      <td>0.485590</td>\n",
       "      <td>0.183473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928902</td>\n",
       "      <td>0.988580</td>\n",
       "      <td>0.675213</td>\n",
       "      <td>0.990091</td>\n",
       "      <td>0.993232</td>\n",
       "      <td>0.954224</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.106807</td>\n",
       "      <td>0.949878</td>\n",
       "      <td>0.933668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>257</td>\n",
       "      <td>0.895700</td>\n",
       "      <td>0.486271</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.739863</td>\n",
       "      <td>0.893735</td>\n",
       "      <td>0.538539</td>\n",
       "      <td>0.457449</td>\n",
       "      <td>0.074345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>0.954958</td>\n",
       "      <td>0.695134</td>\n",
       "      <td>0.972706</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.906162</td>\n",
       "      <td>0.864686</td>\n",
       "      <td>0.182657</td>\n",
       "      <td>0.509210</td>\n",
       "      <td>0.922592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267</td>\n",
       "      <td>0.967291</td>\n",
       "      <td>0.712311</td>\n",
       "      <td>0.267039</td>\n",
       "      <td>0.739393</td>\n",
       "      <td>0.795763</td>\n",
       "      <td>0.796656</td>\n",
       "      <td>0.677582</td>\n",
       "      <td>0.649118</td>\n",
       "      <td>0.201059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906255</td>\n",
       "      <td>0.908691</td>\n",
       "      <td>0.674458</td>\n",
       "      <td>0.960482</td>\n",
       "      <td>0.976234</td>\n",
       "      <td>0.818040</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.976141</td>\n",
       "      <td>0.915883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>284</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>0.445296</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.724650</td>\n",
       "      <td>0.789642</td>\n",
       "      <td>0.890839</td>\n",
       "      <td>0.530727</td>\n",
       "      <td>0.467378</td>\n",
       "      <td>0.412352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803386</td>\n",
       "      <td>0.972898</td>\n",
       "      <td>0.646042</td>\n",
       "      <td>0.983505</td>\n",
       "      <td>0.991843</td>\n",
       "      <td>0.919998</td>\n",
       "      <td>0.803075</td>\n",
       "      <td>0.159615</td>\n",
       "      <td>0.551880</td>\n",
       "      <td>0.917416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>292</td>\n",
       "      <td>0.969639</td>\n",
       "      <td>0.732235</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.911313</td>\n",
       "      <td>0.860330</td>\n",
       "      <td>0.687019</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>0.061705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913862</td>\n",
       "      <td>0.876258</td>\n",
       "      <td>0.632740</td>\n",
       "      <td>0.962797</td>\n",
       "      <td>0.977443</td>\n",
       "      <td>0.785966</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>0.109399</td>\n",
       "      <td>0.727858</td>\n",
       "      <td>0.927811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.950409                0.667482   \n",
       "1     46                             0.859322                0.471540   \n",
       "2     70                             0.908096                0.620519   \n",
       "3    132                             0.894989                0.414773   \n",
       "4    200                             0.921447                0.422136   \n",
       "5    245                             0.949044                0.838918   \n",
       "6    257                             0.895700                0.486271   \n",
       "7    267                             0.967291                0.712311   \n",
       "8    284                             0.892987                0.445296   \n",
       "9    292                             0.969639                0.732235   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.239416                      0.339715   \n",
       "1                 0.005667                      0.772744   \n",
       "2                 0.014831                      0.787047   \n",
       "3                 0.009222                      0.709674   \n",
       "4                 0.035422                      0.858205   \n",
       "5                 0.037897                      0.670364   \n",
       "6                 0.006919                      0.726316   \n",
       "7                 0.267039                      0.739393   \n",
       "8                 0.006820                      0.724650   \n",
       "9                 0.017571                      0.876691   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.570079                               0.466379   \n",
       "1               0.760206                               0.914244   \n",
       "2               0.934155                               0.962700   \n",
       "3               0.744439                               0.904797   \n",
       "4               0.736690                               0.841480   \n",
       "5               0.950395                               0.876493   \n",
       "6               0.739863                               0.893735   \n",
       "7               0.795763                               0.796656   \n",
       "8               0.789642                               0.890839   \n",
       "9               0.911313                               0.860330   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.700281                       0.700085   \n",
       "1                         0.574795                       0.459414   \n",
       "2                         0.608684                       0.413993   \n",
       "3                         0.542779                       0.426597   \n",
       "4                         0.635920                       0.601400   \n",
       "5                         0.649106                       0.485590   \n",
       "6                         0.538539                       0.457449   \n",
       "7                         0.677582                       0.649118   \n",
       "8                         0.530727                       0.467378   \n",
       "9                         0.687019                       0.573530   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.571651  ...               0.939537        0.927281   \n",
       "1               0.150252  ...               0.614215        0.965300   \n",
       "2               0.300243  ...               0.878956        0.914537   \n",
       "3               0.098071  ...               0.731927        0.948509   \n",
       "4               0.237748  ...               0.662975        0.926064   \n",
       "5               0.183473  ...               0.928902        0.988580   \n",
       "6               0.074345  ...               0.751802        0.954958   \n",
       "7               0.201059  ...               0.906255        0.908691   \n",
       "8               0.412352  ...               0.803386        0.972898   \n",
       "9               0.061705  ...               0.913862        0.876258   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.502933          0.972254          0.975164   \n",
       "1                     0.622804          0.984339          0.987231   \n",
       "2                     0.562891          0.970561          0.969091   \n",
       "3                     0.699444          0.967164          0.983113   \n",
       "4                     0.677840          0.978456          0.973675   \n",
       "5                     0.675213          0.990091          0.993232   \n",
       "6                     0.695134          0.972706          0.987342   \n",
       "7                     0.674458          0.960482          0.976234   \n",
       "8                     0.646042          0.983505          0.991843   \n",
       "9                     0.632740          0.962797          0.977443   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.780713                  0.031082               0.050007   \n",
       "1             0.901024                  0.941507               0.119569   \n",
       "2             0.786442                  0.036871               0.066868   \n",
       "3             0.905921                  0.857659               0.183874   \n",
       "4             0.848344                  0.341229               0.161605   \n",
       "5             0.954224                  0.005941               0.106807   \n",
       "6             0.906162                  0.864686               0.182657   \n",
       "7             0.818040                  0.005299               0.006220   \n",
       "8             0.919998                  0.803075               0.159615   \n",
       "9             0.785966                  0.316395               0.109399   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.861596             0.923908  \n",
       "1                        0.096480             0.902197  \n",
       "2                        0.924306             0.903338  \n",
       "3                        0.640012             0.921505  \n",
       "4                        0.517119             0.905178  \n",
       "5                        0.949878             0.933668  \n",
       "6                        0.509210             0.922592  \n",
       "7                        0.976141             0.915883  \n",
       "8                        0.551880             0.917416  \n",
       "9                        0.727858             0.927811  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##post process\n",
    "# cols2process = df_train.columns.tolist()[11:]\n",
    "# print(len(cols2process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in tqdm(cols2process):\n",
    "#     ###step1\n",
    "#     quantiles = df_train[col].value_counts()/len(df_train.index)\n",
    "#     quantiles = quantiles.to_dict()\n",
    "#     quantiles = {k: v for k, v in sorted(quantiles.items(), key=lambda item: item[0])}\n",
    "#     ks = list(quantiles.keys())\n",
    "#     vs = list(quantiles.values())\n",
    "#     qs = np.cumsum(vs)\n",
    "#     #print(ks)\n",
    "#     #print(qs)\n",
    "#     ###step2\n",
    "#     qs = np.quantile(df_sub[col], qs)\n",
    "#     #print(qs)\n",
    "#     for i in range(len(qs)):\n",
    "#         if i==0:\n",
    "#             q = qs[0]\n",
    "#             df_sub.loc[df_sub[col]<q, col] = ks[i]\n",
    "#         elif i>0 and i<=len(qs)-1:\n",
    "#             q0,q1 = qs[i-1], qs[i]\n",
    "#             df_sub.loc[(df_sub[col]<q1)&(df_sub[col]>=q0), col] = ks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "431ef7cf22944c35b5d6943d404aac43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4b9d2c357ffa481982fd53480dd138d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9c558836a72e49079836e4a6a43bdd8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9e90424a903346d085f829a2ab232047",
        "IPY_MODEL_c256ad9e9e484427a8898242fe0c91af"
       ],
       "layout": "IPY_MODEL_ecb7835103cf451b99272d475a2a671a"
      }
     },
     "9e90424a903346d085f829a2ab232047": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a413da8679874ac7a18608dd20db956f",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_431ef7cf22944c35b5d6943d404aac43",
       "value": 1
      }
     },
     "a413da8679874ac7a18608dd20db956f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c256ad9e9e484427a8898242fe0c91af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d81c6ff316ed4c7fabb06907f9caf791",
       "placeholder": "",
       "style": "IPY_MODEL_4b9d2c357ffa481982fd53480dd138d3",
       "value": " 476/? [00:07&lt;00:00, 67.16it/s]"
      }
     },
     "d81c6ff316ed4c7fabb06907f9caf791": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ecb7835103cf451b99272d475a2a671a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
